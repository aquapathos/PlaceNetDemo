{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一般物体認識\n",
    " |||\n",
    " |:--:|:--:|:--:|\n",
    " |![](https://user-images.githubusercontent.com/5820803/29510276-2bed3c18-8696-11e7-8b7b-001e6fcdfd97.PNG)|![](https://user-images.githubusercontent.com/5820803/29509773-518d1d6e-8694-11e7-8a8d-b508c99eb7da.PNG)|\n",
    "\n",
    "## 使い方\n",
    "1. まず、下の箱の中でShift キーを押しながら Enterを押してください。\n",
    "2. url の箱の中に認識させたい画像の URL をはりつけてください。\n",
    "\n",
    "### 画像のURLについて\n",
    "<a href=\"https://goo.gl/uto24t\" target=\"_blank\">Google 画像検索</a>\n",
    "(右クリックして新しいウィンドウを開いてください)　などで画像を探してください。\n",
    "Google の場合、画像をひとつ選んでクリック、画像右横に出る　** 「画像を表示」 ** のボタンを押してその画像のみを表示し、ブラウザ上部に表示されている URL をコピーしてください。\n",
    " \n",
    " URL の末尾が jpg や png などの画像の拡張子であることを確認してから下の url の箱にペーストしてください。\n",
    " サイトによっては画像へのアクセスが拒否される場合もあります。エラーが出た場合は別の URL  でチャレンジしてみてください。\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53921f5f0766475582e0b66a4d0e3bc4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from placenetVgg import predict\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "interact(predict, url=\"https://goo.gl/FRCv7R\", trans=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写真や絵の中に写っているものが何かをこたえることは皆さんには特に難しいことではありませんね。\n",
    "\n",
    "わたしたちはパッと一目見ただけで見たものが何であるかを認識できます。例えば下の写真がどういうシーンを写したものか、誰でもすぐにわかります。\n",
    "\n",
    "人がホームを歩いていることや、電車が停まっていることも、小さくて見えないけども時計があることもわかりますし、一番手前の人がおそらく女性であり、帽子を被っていること、傘を持っていること、だからこの日は雨だったのかな、というところまでわかります。\n",
    "\n",
    "このようなものの認識は**一般物体認識**とよばれます。工場で決まったものをを工作するロボットや、ロボットサッカーのロボットは、それはそれでとても難しい技術から成り立っていますが、彼らは特定のものだけを見て認識したらいいだけなので、「認識」処理自体は一般物体認識と比べればとても簡単です。（それでも十分難しい。）　\n",
    "\n",
    "最近よく話題になる自動運転でさえ、一般物体認識と比べればずいぶん簡単です。道路上にあるものだけを対象とすればいいですし、障害物かそうでないか、それがどう障害になるかだけわかればいいですから。\n",
    "\n",
    "これまでコンピュータは**パターン認識**が苦手だとされてきました。とりわけ一般物体認識は苦手でした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＃　ソースコード   placenetVgg.py\n",
    "```\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "from chainer import Variable\n",
    "# from chainer.links import caffe\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import io\n",
    "import urllib.request\n",
    "from chainer.links.model.vision.vgg import prepare as VGGprepare\n",
    "\n",
    "from IPython.display import clear_output, Image, display\n",
    "import pickle\n",
    "\n",
    "mean = np.array([103.939, 116.779, 123.68])   # BGR\n",
    "# blob データを PIL 画像に変換\n",
    "def blob2img(blob, mean=mean):\n",
    "    blob = (np.dstack(blob)+ mean)[:,:,::-1]   # BGR 2 RGB\n",
    "    return PIL.Image.fromarray(np.uint8(blob))\n",
    "\n",
    "# Google 翻訳サービスを使う　ーーーーーーーーーーーーーーーーーーー\n",
    "import requests\n",
    "import re\n",
    " \n",
    "url = 'https://translate.google.com/?hl=ja#en/ja/'\n",
    " \n",
    "def translate(estring):\n",
    "    r = requests.get(url, params={'q': estring})\n",
    " \n",
    "    pattern = \"TRANSLATED_TEXT=\\'(.*?)\\'\"\n",
    "    jstring = re.search(pattern, r.text).group(1)\n",
    " \n",
    "    return jstring\n",
    "# Google 翻訳サービスを使う　ーーーーーーーーーーーーーーーーーーー\n",
    "\n",
    "# 確率リストとしての出力からトップ５を出力するメソッドーーーーーーーーーーーーー\n",
    "# ILSVSR１２のカテゴリデータ　　https://raw.githubusercontent.com/CSAILVision/places365/master/categories_hybrid1365.txt\n",
    "categories = np.loadtxt(\"categories.txt\",str,delimiter='\\t')\n",
    "def showtop(prob, ranklimit=5, trans=True): # prob は最終層から出力される確率リスト（Variable構造体)\n",
    "    top5args = np.argsort(prob.data)[:-ranklimit-1:-1] # 上位５つの番号\n",
    "    top5probs = prob.data[top5args] # 上位５つの確率\n",
    "    for rank,(p,words) in enumerate(zip(top5probs,categories[top5args])):\n",
    "        if trans:\n",
    "            print(rank+1,translate(words[2:-1]))   #  on mac\n",
    "        else:\n",
    "            print(rank+1,words[2:-1])\n",
    "        # print(rank,translate(words)\n",
    "# 確率リストとしての出力からトップ５を出力するメソッドーーーーーーーーーーーーー\n",
    "\n",
    "# caffemodelを読み込む\n",
    "# model = caffe.CaffeFunction('modeldata/vgg16_hybrid1365.caffemodel')\n",
    "\n",
    "vgg = pickle.load(open('modeldata/vgg16_hybrid1365.pkl', 'rb'))\n",
    "\n",
    "def url2img(url):\n",
    "    print(url)\n",
    "    f = io.BytesIO(urllib.request.urlopen(url).read())\n",
    "    img = PIL.Image.open(f)\n",
    "    w,h = img.width, img.height\n",
    "    if w > h:\n",
    "        w1, h1 = int(224/h * w), 224\n",
    "    else :\n",
    "        w1,h1 = 224, int(224/w * h)\n",
    "    return img.resize((w1,h1))\n",
    "    \n",
    "def predict(url=\"\", trans=True):\n",
    "    img = url2img(url)\n",
    "    x = Variable( VGGprepare(img)[np.newaxis,])\n",
    "    y, = vgg(inputs={'data': x}, outputs=['fc8a'])\n",
    "    predict = F.softmax(y)\n",
    "    showtop(predict[0], trans=trans)\n",
    "    return img\n",
    "```\n",
    "\n",
    "### 学習済みモデル\n",
    "このプログラムでは、MITが後悔している　　place365hybrid という、一般の物体とシーンを対象とした学習済み CNN(畳み込みネットワーク）を使っています。　https://github.com/CSAILVision/places365\n",
    "\n",
    "このネットワークは、、一般物体認識は ILSCVR とよばれるコンテストで用いられている 1000カテゴリと Place Net の 365のカテゴリを合わせた 1365 のカテゴリについて、与えられた画像の被写体である確率を出力します。このプログラムでは確率の上位5つを表示しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gpu = 0\n",
    "model = myDNN(100, 500, 10)\n",
    "chainer.cuda.get_device(gpu).use()\n",
    "model.to_gpu(gpu) # GPUを使うための処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import placenetVgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import serializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3558939fe804f9e8899833010498538"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
